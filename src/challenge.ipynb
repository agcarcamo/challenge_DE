{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para resolver los 3 problemas se tomó la opción de abordar con Bigquery los casos en donde se busca reducir los tiempos de ejecucion, esto es, para q1_time, q2_time y q3_time. Si bien para el caso de pruebas que contempla un archivo con 117408 registros no justifica su uso, pensando en un volumen mayor de registro (millones) si lo justificaria por sus caracteristicas propias, paralelización de consultas en los nodos de procesamiento y optimización de ejecución.\n",
    "Por su parte, para los casos q1_memory, q2_memory y q3_memory donde se busca optimizar el uso de memoria, se ha optado por el uso de pyspak, por su enfoque de procesamiento basado en particiones, que puede dividir los datos en bloques y procesarlos en paralelo."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PROBLEMA 1: Las top 10 fechas donde hay más tweets\n",
    "\n",
    "### q1_time\n",
    "\n",
    "\n",
    "Lo primero que se hace es cargar la data en una tabla en bigquery.\n",
    "\n",
    "Se indica un dataset y un nombre de tabla donde será cargada la data. Las variables se encuentran seteadas dentro de la función (primeras lineas).\n",
    "\n",
    "Una vez cargada la data, se ejecuta la query contenida en el archivo q1_time.sql en el folder \"queries\".\n",
    "\n",
    "La logica de esta query es primero obtener las 10 fechas con mas tweets\n",
    "\n",
    "    SELECT DATE(date)\n",
    "    FROM {dataset_id}.{table_id}\n",
    "    GROUP BY DATE(date)\n",
    "    ORDER BY COUNT(1) DESC\n",
    "    LIMIT 10\n",
    "\n",
    "Luego, obtiene el numero de tweets por cada usuario pero solo sobre las top 10 fechas ya obtenidas.\n",
    "\n",
    "    SELECT\n",
    "    DATE(date) AS tweet_date,\n",
    "    user.username AS username,\n",
    "    COUNT(1) AS tweet_count\n",
    "    FROM {dataset_id}.{table_id}\n",
    "    WHERE DATE(date) IN (\n",
    "        -- Subconsulta de las 10 fechas principales\n",
    "    )\n",
    "    GROUP BY tweet_date, username\n",
    "\n",
    "\n",
    "Con la funcion rank() se asigna un rango a cada usuario basado en la cantidad de tweets, dentro de cada fecha.\n",
    "\n",
    "    SELECT\n",
    "    tweet_date,\n",
    "    username,\n",
    "    RANK() OVER (PARTITION BY tweet_date ORDER BY tweet_count DESC) AS rank\n",
    "    FROM (\n",
    "        -- Resultados anteriores\n",
    "    ) AS aggregated_data\n",
    "\n",
    "\n",
    "Finalmente se filtra por rank=1, para obtener el usuario con mas tweets para cada dia.\n",
    "\n",
    "    SELECT\n",
    "    tweet_date,\n",
    "    username\n",
    "    FROM (\n",
    "        -- Resultados anteriores con ranking\n",
    "    )\n",
    "    WHERE rank = 1\n",
    "    ORDER BY tweet_date ASC\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T06:48:43.467004Z",
     "start_time": "2024-11-23T06:47:34.321359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from q1_memory import q1_memory\n",
    "from q1_time import q1_time\n",
    "file_path = \"/Users/acarcamo/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "q1_time(file_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados en Bigquery\n",
      "Filename: /Users/acarcamo/Documents/Personal/challenge_DE/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    11     93.0 MiB     93.0 MiB           1   @profile\n",
      "    12                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    13     93.0 MiB      0.0 MiB           1       dataset_id = \"sandbox_agcarcamo\"\n",
      "    14     93.0 MiB      0.0 MiB           1       table_id = \"de_test5\"\n",
      "    15                                         \n",
      "    16     93.0 MiB      0.0 MiB           1       result = []\n",
      "    17                                         \n",
      "    18     93.0 MiB      0.0 MiB           1       try:\n",
      "    19                                                 # Valida si el archivo existe\n",
      "    20     93.0 MiB      0.0 MiB           1           if not os.path.exists(file_path):\n",
      "    21                                                     raise FileNotFoundError(f\"El archivo {file_path} no se encuentra.\")\n",
      "    22                                         \n",
      "    23     93.4 MiB      0.4 MiB           1           client = bigquery.Client()\n",
      "    24                                         \n",
      "    25                                         \n",
      "    26   1109.6 MiB      0.0 MiB           2           with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
      "    27   1109.6 MiB   1016.2 MiB      117408               json_data = [json.loads(line) for line in file]\n",
      "    28                                         \n",
      "    29                                                 # Configuracion del Job\n",
      "    30   1109.6 MiB      0.0 MiB           2           job_config = bigquery.LoadJobConfig(\n",
      "    31   1109.6 MiB      0.0 MiB           1               source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
      "    32   1109.6 MiB      0.0 MiB           1               autodetect=True,\n",
      "    33   1109.6 MiB      0.0 MiB           1               max_bad_records=10,\n",
      "    34   1109.6 MiB      0.0 MiB           1               write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
      "    35                                                 )\n",
      "    36                                         \n",
      "    37                                                 # Job para cargar la data\n",
      "    38   2543.6 MiB   1433.9 MiB           2           load_job = client.load_table_from_json(\n",
      "    39   1109.6 MiB      0.0 MiB           1               json_data, dataset_id + '.' + table_id, job_config=job_config\n",
      "    40                                                 )\n",
      "    41                                         \n",
      "    42                                                 # Espera a que termine el job en BQ\n",
      "    43   2544.0 MiB      0.4 MiB           1           load_job.result()\n",
      "    44                                         \n",
      "    45   2544.0 MiB      0.0 MiB           1           print(\"Datos cargados en Bigquery\")\n",
      "    46                                         \n",
      "    47                                                 # Busca la query\n",
      "    48   2544.0 MiB      0.0 MiB           1           sql_file_path = os.path.join(os.path.dirname(__file__), \"queries\", \"q1_time.sql\")\n",
      "    49   2544.0 MiB      0.0 MiB           1           if not os.path.exists(sql_file_path):\n",
      "    50                                                     raise FileNotFoundError(f\"El archivo SQL {sql_file_path} no se encuentra.\")\n",
      "    51                                         \n",
      "    52   2544.0 MiB      0.0 MiB           2           with open(sql_file_path, \"r\", encoding=\"utf-8\") as sql_file:\n",
      "    53   2544.0 MiB      0.0 MiB           1               query_template = sql_file.read()\n",
      "    54                                         \n",
      "    55                                                 # Setea dataset_id y table_id en la query\n",
      "    56   2544.0 MiB      0.0 MiB           1           try:\n",
      "    57   2544.0 MiB      0.0 MiB           1               query = query_template.format(dataset_id=dataset_id, table_id=table_id)\n",
      "    58                                                 except KeyError as e:\n",
      "    59                                                     return result\n",
      "    60                                         \n",
      "    61                                         \n",
      "    62   2544.1 MiB      0.1 MiB           1           query_job = client.query(query)\n",
      "    63   2544.1 MiB      0.1 MiB           1           results = query_job.result()\n",
      "    64                                         \n",
      "    65                                                 # Armar lista de salida\n",
      "    66   2544.1 MiB      0.0 MiB          11           for row in results:\n",
      "    67   2544.1 MiB      0.0 MiB          10               username = row['username']\n",
      "    68   2544.1 MiB      0.0 MiB          10               tweet_date = row['tweet_date']\n",
      "    69   2544.1 MiB      0.0 MiB          10               result.append((tweet_date, username))\n",
      "    70                                         \n",
      "    71                                             except FileNotFoundError as e:\n",
      "    72                                                 print(f\"Error: {e}\")\n",
      "    73                                             except GoogleAPIError as e:\n",
      "    74                                                 print(f\"Error en BigQuery: {e}\")\n",
      "    75                                             except Exception as e:\n",
      "    76                                                 print(f\"Error no esperado: {e}\")\n",
      "    77                                         \n",
      "    78   2544.1 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### q1_memory\n",
    "Para este caso, tambien se realiza una prueba local. Queda pendiente realizar la ejecución en un cluster dataproc.\n",
    "\n",
    "Pasos de ejecución:\n",
    "\n",
    "Se carga la data en un dataframe\n",
    "\n",
    "    df = spark.read.option(\"encoding\", \"UTF-8\").json(file_path)\n",
    "\n",
    "\n",
    "Se seleccionan solo las columnas date y user.username necesarias para este caso y se filtran los nulos si es que hubiese.\n",
    "\n",
    "    df = df.select(col(\"date\"), col(\"user.username\")).filter(\n",
    "    col(\"date\").isNotNull() & col(\"username\").isNotNull()\n",
    "    )\n",
    "\n",
    "\n",
    "Convierte el campo date a fecha, formato yyyy-MM-dd\n",
    "\n",
    "    df = df.withColumn(\"tweet_date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "\n",
    "Obtiene count de tweets por dia\n",
    "\n",
    "    date_counts = df.groupBy(\"tweet_date\").agg(count(\"*\").alias(\"tweet_count\"))\n",
    "\n",
    "\n",
    "Obtiene top 10 fechas con mas tweets\n",
    "\n",
    "    top_dates = date_counts.orderBy(col(\"tweet_count\").desc()).limit(10)\n",
    "\n",
    "\n",
    "Obtiene count de tweets de usuario por dia\n",
    "\n",
    "    user_tweet_counts = df.groupBy(\"tweet_date\", \"username\").agg(count(\"*\").alias(\"tweet_count_user\"))\n",
    "\n",
    "\n",
    "Join entre top 10 dias y tweets por usuario en cada dia\n",
    "\n",
    "    top_users = user_tweet_counts.join(broadcast(top_dates), on=\"tweet_date\")\n",
    "\n",
    "\n",
    "Realiza rank de usuarios por tweets\n",
    "\n",
    "    window_spec = Window.partitionBy(\"tweet_date\").orderBy(col(\"tweet_count_user\").desc())\n",
    "    ranked_users = top_users.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "\n",
    "Filtra solo el usuario con mas tweets por dia\n",
    "\n",
    "    result = ranked_users.filter(col(\"rank\") == 1).select(\"tweet_date\", \"username\")\n",
    "\n",
    "\n",
    "Formatea la salida\n",
    "\n",
    "    result_list = result.collect()\n",
    "    formatted_result = [(row[\"tweet_date\"], row[\"username\"]) for row in result_list]\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T06:49:44.266471Z",
     "start_time": "2024-11-23T06:48:58.251835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from q1_memory import q1_memory\n",
    "\n",
    "file_path = \"/Users/acarcamo/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "q1_memory(file_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/23 03:49:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n",
      "[Stage 2:>                (0 + 10) / 10][Stage 3:>                 (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=25225Kb max_used=25225Kb free=105846Kb\n",
      " bounds [0x0000000105e18000, 0x00000001076e8000, 0x000000010de18000]\n",
      " total_blobs=9821 nmethods=8902 adapters=830\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/acarcamo/Documents/Personal/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     9     71.6 MiB     71.6 MiB           1   @profile\n",
      "    10                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    11     71.6 MiB      0.0 MiB           1       spark = None\n",
      "    12     71.6 MiB      0.0 MiB           1       try:\n",
      "    13                                                 # Crea sesión de Spark\n",
      "    14     73.6 MiB      2.0 MiB           1           spark = SparkSession.builder.appName(\"Top10Tweets\").getOrCreate()\n",
      "    15                                         \n",
      "    16                                                 # Carga los datos en dataframe\n",
      "    17     73.6 MiB      0.0 MiB           1           try:\n",
      "    18     73.8 MiB      0.2 MiB           1               df = spark.read.option(\"encoding\", \"UTF-8\").json(file_path)\n",
      "    19                                                 except Exception as e:\n",
      "    20                                                     raise ValueError(f\"Error leyendo el archivo JSON: {e}\")\n",
      "    21                                         \n",
      "    22                                                 # Selecciona solo columnas necesarias y valores no nulos\n",
      "    23     73.8 MiB      0.0 MiB           2           df = df.select(col(\"date\"), col(\"user.username\")).filter(\n",
      "    24     73.8 MiB      0.0 MiB           1               col(\"date\").isNotNull() & col(\"username\").isNotNull()\n",
      "    25                                                 )\n",
      "    26                                                 #df2.printSchema()\n",
      "    27                                         \n",
      "    28                                         \n",
      "    29                                                 # Filtra registros con valores nulos en date y username\n",
      "    30                                                 #df = df.filter(col(\"date\").isNotNull() & col(\"user.username\").isNotNull())\n",
      "    31                                         \n",
      "    32                                                 # Si el dataframe está vacío después del filtrado, retorna una lista vacía\n",
      "    33     74.0 MiB      0.2 MiB           1           if df.rdd.isEmpty():\n",
      "    34                                                     return []\n",
      "    35                                         \n",
      "    36                                                 # se crea columna tweet_date como tipo date\n",
      "    37     74.0 MiB      0.0 MiB           1           df = df.withColumn(\"tweet_date\", col(\"date\").cast(\"date\"))\n",
      "    38                                         \n",
      "    39                                                 # Obtiene Cantidad de tweets por día\n",
      "    40     74.0 MiB      0.0 MiB           1           date_counts = df.groupBy(\"tweet_date\").agg(count(\"*\").alias(\"tweet_count\"))\n",
      "    41                                         \n",
      "    42                                                 # Obiene Top 10 fechas con más tweets\n",
      "    43     74.0 MiB      0.0 MiB           1           top_dates = date_counts.orderBy(col(\"tweet_count\").desc()).limit(10)\n",
      "    44                                         \n",
      "    45                                                 # Obtiene numero de Tweets por usuario en cada día\n",
      "    46     74.0 MiB      0.0 MiB           1           user_tweet_counts = df.groupBy(\"tweet_date\", \"username\").agg(count(\"*\").alias(\"tweet_count_user\"))\n",
      "    47                                         \n",
      "    48                                                 # join con top 10 fechas\n",
      "    49     74.0 MiB      0.0 MiB           1           top_users = user_tweet_counts.join(broadcast(top_dates), on=\"tweet_date\")\n",
      "    50                                         \n",
      "    51                                                 # Rank para encontrar el usuario con más tweets en cada fecha\n",
      "    52     74.0 MiB      0.0 MiB           1           window_spec = Window.partitionBy(\"tweet_date\").orderBy(col(\"tweet_count_user\").desc())\n",
      "    53     74.0 MiB      0.0 MiB           1           ranked_users = top_users.withColumn(\"rank\", rank().over(window_spec))\n",
      "    54                                         \n",
      "    55                                                 # Usuario con más tweets por fecha\n",
      "    56     74.1 MiB      0.0 MiB           1           result = ranked_users.filter(col(\"rank\") == 1).select(\"tweet_date\", \"username\", \"tweet_count_user\")\n",
      "    57                                         \n",
      "    58                                                 # Armar lista de tuplas\n",
      "    59     74.7 MiB      0.7 MiB           1           result_list = result.take(10)  # Solo tomar las primeras 10 filas\n",
      "    60                                         \n",
      "    61                                                 # Formatear la salida para que sea como una lista de tuplas con la fecha y el usuario\n",
      "    62     74.7 MiB      0.0 MiB          11           formatted_result = [(row[\"tweet_date\"], row[\"username\"]) for row in result_list]\n",
      "    63                                         \n",
      "    64     74.7 MiB      0.0 MiB           1           return formatted_result\n",
      "    65                                         \n",
      "    66                                             except Exception as e:\n",
      "    67                                                 print(f\"Error procesando los datos: {e}\")\n",
      "    68                                                 return []\n",
      "    69                                             finally:\n",
      "    70     74.7 MiB      0.0 MiB           1           if spark is not None:\n",
      "    71     74.7 MiB     -0.1 MiB           1               spark.stop()\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
