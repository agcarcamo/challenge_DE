{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para resolver los 3 problemas se tomó la opción de abordar con Bigquery los casos en donde se busca reducir los tiempos de ejecucion, esto es, para q1_time, q2_time y q3_time. Si bien para el caso de pruebas que contempla un archivo con 117408 registros no justifica su uso, pensando en un volumen mayor de registro (millones) si lo justificaria por sus caracteristicas propias, paralelización de consultas en los nodos de procesamiento y optimización de ejecución.\n",
    "Por su parte, para los casos q1_memory, q2_memory y q3_memory donde se busca optimizar el uso de memoria, se ha optado por el uso de pyspak, por su enfoque de procesamiento basado en particiones, que puede dividir los datos en bloques y procesarlos en paralelo."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PROBLEMA 1: Las top 10 fechas donde hay más tweets\n",
    "\n",
    "### q1_time\n",
    "\n",
    "\n",
    "Lo primero que se hace es cargar la data en una tabla en bigquery.\n",
    "\n",
    "Se indica un dataset y un nombre de tabla donde será cargada la data. Las variables se encuentran seteadas dentro de la función (primeras lineas).\n",
    "\n",
    "Una vez cargada la data, se ejecuta la query contenida en el archivo q1_time.sql en el folder \"queries\".\n",
    "\n",
    "La logica de esta query es primero obtener las 10 fechas con mas tweets\n",
    "\n",
    "    SELECT DATE(date)\n",
    "    FROM {dataset_id}.{table_id}\n",
    "    GROUP BY DATE(date)\n",
    "    ORDER BY COUNT(1) DESC\n",
    "    LIMIT 10\n",
    "\n",
    "Luego, obtiene el numero de tweets por cada usuario pero solo sobre las top 10 fechas ya obtenidas.\n",
    "\n",
    "    SELECT\n",
    "    DATE(date) AS tweet_date,\n",
    "    user.username AS username,\n",
    "    COUNT(1) AS tweet_count\n",
    "    FROM {dataset_id}.{table_id}\n",
    "    WHERE DATE(date) IN (\n",
    "        -- Subconsulta de las 10 fechas principales\n",
    "    )\n",
    "    GROUP BY tweet_date, username\n",
    "\n",
    "\n",
    "Con la funcion rank() se asigna un rango a cada usuario basado en la cantidad de tweets, dentro de cada fecha.\n",
    "\n",
    "    SELECT\n",
    "    tweet_date,\n",
    "    username,\n",
    "    RANK() OVER (PARTITION BY tweet_date ORDER BY tweet_count DESC) AS rank\n",
    "    FROM (\n",
    "        -- Resultados anteriores\n",
    "    ) AS aggregated_data\n",
    "\n",
    "\n",
    "Finalmente se filtra por rank=1, para obtener el usuario con mas tweets para cada dia.\n",
    "\n",
    "    SELECT\n",
    "    tweet_date,\n",
    "    username\n",
    "    FROM (\n",
    "        -- Resultados anteriores con ranking\n",
    "    )\n",
    "    WHERE rank = 1\n",
    "    ORDER BY tweet_date ASC\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T06:48:43.467004Z",
     "start_time": "2024-11-23T06:47:34.321359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from q1_memory import q1_memory\n",
    "from q1_time import q1_time\n",
    "file_path = \"/Users/acarcamo/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "q1_time(file_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados en Bigquery\n",
      "Filename: /Users/acarcamo/Documents/Personal/challenge_DE/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    11     93.0 MiB     93.0 MiB           1   @profile\n",
      "    12                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    13     93.0 MiB      0.0 MiB           1       dataset_id = \"sandbox_agcarcamo\"\n",
      "    14     93.0 MiB      0.0 MiB           1       table_id = \"de_test5\"\n",
      "    15                                         \n",
      "    16     93.0 MiB      0.0 MiB           1       result = []\n",
      "    17                                         \n",
      "    18     93.0 MiB      0.0 MiB           1       try:\n",
      "    19                                                 # Valida si el archivo existe\n",
      "    20     93.0 MiB      0.0 MiB           1           if not os.path.exists(file_path):\n",
      "    21                                                     raise FileNotFoundError(f\"El archivo {file_path} no se encuentra.\")\n",
      "    22                                         \n",
      "    23     93.4 MiB      0.4 MiB           1           client = bigquery.Client()\n",
      "    24                                         \n",
      "    25                                         \n",
      "    26   1109.6 MiB      0.0 MiB           2           with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
      "    27   1109.6 MiB   1016.2 MiB      117408               json_data = [json.loads(line) for line in file]\n",
      "    28                                         \n",
      "    29                                                 # Configuracion del Job\n",
      "    30   1109.6 MiB      0.0 MiB           2           job_config = bigquery.LoadJobConfig(\n",
      "    31   1109.6 MiB      0.0 MiB           1               source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
      "    32   1109.6 MiB      0.0 MiB           1               autodetect=True,\n",
      "    33   1109.6 MiB      0.0 MiB           1               max_bad_records=10,\n",
      "    34   1109.6 MiB      0.0 MiB           1               write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
      "    35                                                 )\n",
      "    36                                         \n",
      "    37                                                 # Job para cargar la data\n",
      "    38   2543.6 MiB   1433.9 MiB           2           load_job = client.load_table_from_json(\n",
      "    39   1109.6 MiB      0.0 MiB           1               json_data, dataset_id + '.' + table_id, job_config=job_config\n",
      "    40                                                 )\n",
      "    41                                         \n",
      "    42                                                 # Espera a que termine el job en BQ\n",
      "    43   2544.0 MiB      0.4 MiB           1           load_job.result()\n",
      "    44                                         \n",
      "    45   2544.0 MiB      0.0 MiB           1           print(\"Datos cargados en Bigquery\")\n",
      "    46                                         \n",
      "    47                                                 # Busca la query\n",
      "    48   2544.0 MiB      0.0 MiB           1           sql_file_path = os.path.join(os.path.dirname(__file__), \"queries\", \"q1_time.sql\")\n",
      "    49   2544.0 MiB      0.0 MiB           1           if not os.path.exists(sql_file_path):\n",
      "    50                                                     raise FileNotFoundError(f\"El archivo SQL {sql_file_path} no se encuentra.\")\n",
      "    51                                         \n",
      "    52   2544.0 MiB      0.0 MiB           2           with open(sql_file_path, \"r\", encoding=\"utf-8\") as sql_file:\n",
      "    53   2544.0 MiB      0.0 MiB           1               query_template = sql_file.read()\n",
      "    54                                         \n",
      "    55                                                 # Setea dataset_id y table_id en la query\n",
      "    56   2544.0 MiB      0.0 MiB           1           try:\n",
      "    57   2544.0 MiB      0.0 MiB           1               query = query_template.format(dataset_id=dataset_id, table_id=table_id)\n",
      "    58                                                 except KeyError as e:\n",
      "    59                                                     return result\n",
      "    60                                         \n",
      "    61                                         \n",
      "    62   2544.1 MiB      0.1 MiB           1           query_job = client.query(query)\n",
      "    63   2544.1 MiB      0.1 MiB           1           results = query_job.result()\n",
      "    64                                         \n",
      "    65                                                 # Armar lista de salida\n",
      "    66   2544.1 MiB      0.0 MiB          11           for row in results:\n",
      "    67   2544.1 MiB      0.0 MiB          10               username = row['username']\n",
      "    68   2544.1 MiB      0.0 MiB          10               tweet_date = row['tweet_date']\n",
      "    69   2544.1 MiB      0.0 MiB          10               result.append((tweet_date, username))\n",
      "    70                                         \n",
      "    71                                             except FileNotFoundError as e:\n",
      "    72                                                 print(f\"Error: {e}\")\n",
      "    73                                             except GoogleAPIError as e:\n",
      "    74                                                 print(f\"Error en BigQuery: {e}\")\n",
      "    75                                             except Exception as e:\n",
      "    76                                                 print(f\"Error no esperado: {e}\")\n",
      "    77                                         \n",
      "    78   2544.1 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### q1_memory\n",
    "Para este caso, tambien se realiza una prueba local. Queda pendiente realizar la ejecución en un cluster dataproc.\n",
    "\n",
    "Pasos de ejecución:\n",
    "\n",
    "Se carga la data en un dataframe\n",
    "\n",
    "    df = spark.read.option(\"encoding\", \"UTF-8\").json(file_path)\n",
    "\n",
    "\n",
    "Se seleccionan solo las columnas date y user.username necesarias para este caso y se filtran los nulos si es que hubiese.\n",
    "\n",
    "    df = df.select(col(\"date\"), col(\"user.username\")).filter(\n",
    "    col(\"date\").isNotNull() & col(\"username\").isNotNull()\n",
    "    )\n",
    "\n",
    "\n",
    "Convierte el campo date a fecha, formato yyyy-MM-dd\n",
    "\n",
    "    df = df.withColumn(\"tweet_date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "\n",
    "Obtiene count de tweets por dia\n",
    "\n",
    "    date_counts = df.groupBy(\"tweet_date\").agg(count(\"*\").alias(\"tweet_count\"))\n",
    "\n",
    "\n",
    "Obtiene top 10 fechas con mas tweets\n",
    "\n",
    "    top_dates = date_counts.orderBy(col(\"tweet_count\").desc()).limit(10)\n",
    "\n",
    "\n",
    "Obtiene count de tweets de usuario por dia\n",
    "\n",
    "    user_tweet_counts = df.groupBy(\"tweet_date\", \"username\").agg(count(\"*\").alias(\"tweet_count_user\"))\n",
    "\n",
    "\n",
    "Join entre top 10 dias y tweets por usuario en cada dia\n",
    "\n",
    "    top_users = user_tweet_counts.join(broadcast(top_dates), on=\"tweet_date\")\n",
    "\n",
    "\n",
    "Realiza rank de usuarios por tweets\n",
    "\n",
    "    window_spec = Window.partitionBy(\"tweet_date\").orderBy(col(\"tweet_count_user\").desc())\n",
    "    ranked_users = top_users.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "\n",
    "Filtra solo el usuario con mas tweets por dia\n",
    "\n",
    "    result = ranked_users.filter(col(\"rank\") == 1).select(\"tweet_date\", \"username\")\n",
    "\n",
    "\n",
    "Formatea la salida\n",
    "\n",
    "    result_list = result.collect()\n",
    "    formatted_result = [(row[\"tweet_date\"], row[\"username\"]) for row in result_list]\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T06:49:44.266471Z",
     "start_time": "2024-11-23T06:48:58.251835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from q1_memory import q1_memory\n",
    "\n",
    "file_path = \"/Users/acarcamo/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "q1_memory(file_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/23 03:49:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n",
      "[Stage 2:>                (0 + 10) / 10][Stage 3:>                 (0 + 0) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=25225Kb max_used=25225Kb free=105846Kb\n",
      " bounds [0x0000000105e18000, 0x00000001076e8000, 0x000000010de18000]\n",
      " total_blobs=9821 nmethods=8902 adapters=830\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/acarcamo/Documents/Personal/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     9     71.6 MiB     71.6 MiB           1   @profile\n",
      "    10                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    11     71.6 MiB      0.0 MiB           1       spark = None\n",
      "    12     71.6 MiB      0.0 MiB           1       try:\n",
      "    13                                                 # Crea sesión de Spark\n",
      "    14     73.6 MiB      2.0 MiB           1           spark = SparkSession.builder.appName(\"Top10Tweets\").getOrCreate()\n",
      "    15                                         \n",
      "    16                                                 # Carga los datos en dataframe\n",
      "    17     73.6 MiB      0.0 MiB           1           try:\n",
      "    18     73.8 MiB      0.2 MiB           1               df = spark.read.option(\"encoding\", \"UTF-8\").json(file_path)\n",
      "    19                                                 except Exception as e:\n",
      "    20                                                     raise ValueError(f\"Error leyendo el archivo JSON: {e}\")\n",
      "    21                                         \n",
      "    22                                                 # Selecciona solo columnas necesarias y valores no nulos\n",
      "    23     73.8 MiB      0.0 MiB           2           df = df.select(col(\"date\"), col(\"user.username\")).filter(\n",
      "    24     73.8 MiB      0.0 MiB           1               col(\"date\").isNotNull() & col(\"username\").isNotNull()\n",
      "    25                                                 )\n",
      "    26                                                 #df2.printSchema()\n",
      "    27                                         \n",
      "    28                                         \n",
      "    29                                                 # Filtra registros con valores nulos en date y username\n",
      "    30                                                 #df = df.filter(col(\"date\").isNotNull() & col(\"user.username\").isNotNull())\n",
      "    31                                         \n",
      "    32                                                 # Si el dataframe está vacío después del filtrado, retorna una lista vacía\n",
      "    33     74.0 MiB      0.2 MiB           1           if df.rdd.isEmpty():\n",
      "    34                                                     return []\n",
      "    35                                         \n",
      "    36                                                 # se crea columna tweet_date como tipo date\n",
      "    37     74.0 MiB      0.0 MiB           1           df = df.withColumn(\"tweet_date\", col(\"date\").cast(\"date\"))\n",
      "    38                                         \n",
      "    39                                                 # Obtiene Cantidad de tweets por día\n",
      "    40     74.0 MiB      0.0 MiB           1           date_counts = df.groupBy(\"tweet_date\").agg(count(\"*\").alias(\"tweet_count\"))\n",
      "    41                                         \n",
      "    42                                                 # Obiene Top 10 fechas con más tweets\n",
      "    43     74.0 MiB      0.0 MiB           1           top_dates = date_counts.orderBy(col(\"tweet_count\").desc()).limit(10)\n",
      "    44                                         \n",
      "    45                                                 # Obtiene numero de Tweets por usuario en cada día\n",
      "    46     74.0 MiB      0.0 MiB           1           user_tweet_counts = df.groupBy(\"tweet_date\", \"username\").agg(count(\"*\").alias(\"tweet_count_user\"))\n",
      "    47                                         \n",
      "    48                                                 # join con top 10 fechas\n",
      "    49     74.0 MiB      0.0 MiB           1           top_users = user_tweet_counts.join(broadcast(top_dates), on=\"tweet_date\")\n",
      "    50                                         \n",
      "    51                                                 # Rank para encontrar el usuario con más tweets en cada fecha\n",
      "    52     74.0 MiB      0.0 MiB           1           window_spec = Window.partitionBy(\"tweet_date\").orderBy(col(\"tweet_count_user\").desc())\n",
      "    53     74.0 MiB      0.0 MiB           1           ranked_users = top_users.withColumn(\"rank\", rank().over(window_spec))\n",
      "    54                                         \n",
      "    55                                                 # Usuario con más tweets por fecha\n",
      "    56     74.1 MiB      0.0 MiB           1           result = ranked_users.filter(col(\"rank\") == 1).select(\"tweet_date\", \"username\", \"tweet_count_user\")\n",
      "    57                                         \n",
      "    58                                                 # Armar lista de tuplas\n",
      "    59     74.7 MiB      0.7 MiB           1           result_list = result.take(10)  # Solo tomar las primeras 10 filas\n",
      "    60                                         \n",
      "    61                                                 # Formatear la salida para que sea como una lista de tuplas con la fecha y el usuario\n",
      "    62     74.7 MiB      0.0 MiB          11           formatted_result = [(row[\"tweet_date\"], row[\"username\"]) for row in result_list]\n",
      "    63                                         \n",
      "    64     74.7 MiB      0.0 MiB           1           return formatted_result\n",
      "    65                                         \n",
      "    66                                             except Exception as e:\n",
      "    67                                                 print(f\"Error procesando los datos: {e}\")\n",
      "    68                                                 return []\n",
      "    69                                             finally:\n",
      "    70     74.7 MiB      0.0 MiB           1           if spark is not None:\n",
      "    71     74.7 MiB     -0.1 MiB           1               spark.stop()\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PROBLEMA 2: Los top 10 emojis más usados con su respectivo conteo\n",
    "\n",
    "\n",
    "### q2_time\n",
    "\n",
    "\n",
    "Tal como el problema q1_time, primero la data se carga en una tabla bigquery y luego se realiza una query para obtener los emojis mas usados.\n",
    "\n",
    "Tras revisar la data previamente, la forma de abordar este problema fue identificar el rango Unicode donde se definen los emojis,\n",
    "\n",
    "'[\\x{{1F600}}-\\x{{1F64F}}]|[\\x{{1F300}}-\\x{{1F5FF}}]|[\\x{{1F680}}-\\x{{1F6FF}}]|[\\x{{2600}}-\\x{{26FF}}]|[\\x{{2700}}-\\x{{27BF}}]|[\\x{{1F700}}-\\x{{1F77F}}]|[\\x{{1F780}}-\\x{{1F7FF}}]|[\\x{{1F800}}-\\x{{1F8FF}}]|[\\x{{1F900}}-\\x{{1F9FF}}]|[\\x{{1FA00}}-\\x{{1FA6F}}]|[\\x{{1FA70}}-\\x{{1FAFF}}]'\n",
    "\n",
    "\n",
    "pero dentro de esta revisión tambien se identificó el rango Unicode\n",
    "\\x{1F3FB} a \\x{1F3FF}\n",
    "que se utiliza como modificador de tono de piel en los emojis, entonces por ejemplo, para no considerar esto\n",
    "\n",
    "    💪: tono de piel amarillo.\n",
    "    💪🏻: Tono de piel claro.\n",
    "    💪🏼: Tono de piel claro-medio.\n",
    "    💪🏽: Tono de piel medio.\n",
    "    💪🏾: Tono de piel oscuro-medio.\n",
    "\n",
    "como 5 emojis distintos, si no mas bien como el mismo emoji que se repite 5 veces, decidi omitir dicho rango Unicode del contenido.\n",
    "\n",
    "Entonces la query funciona asi,\n",
    "\n",
    "Se elimina de la columna content cualquier modificador de tono de piel que está representado por los rangos Unicode \\x{1F3FB} a \\x{1F3FF} y se renombra la columna procesada como cleaned_content.\n",
    "\n",
    "Se utiliza REGEXP_EXTRACT_ALL con una expresión regular para diferentes rangos de Unicode donde se definen los emojis (como caras, símbolos, objetos, etc.). Asi se obtiene una lista de todos los emojis encontrados en cleaned_content.\n",
    "\n",
    "Se usa UNNEST para convertir la lista resultante de REGEXP_EXTRACT_ALL en filas individuales, donde cada una contiene un emoji.\n",
    "\n",
    "Se agrupa por cada emoji y cuenta cuántas veces aparece cada uno (COUNT(*)).\n",
    "\n",
    "Finalmente, ordena los emojis por su frecuencia en orden descendente (ORDER BY count DESC) y retorna los 10 emojis más usados (LIMIT 10).\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T18:13:21.994482Z",
     "start_time": "2024-11-23T18:12:34.543654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from q2_time import q2_time\n",
    "\n",
    "file_path = \"/Users/acarcamo/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "q2_time(file_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados en Bigquery\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('🙏', 7286),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('✊', 2411),\n",
       " ('🌾', 2363),\n",
       " ('❤', 1779),\n",
       " ('🤣', 1668),\n",
       " ('👇', 1108),\n",
       " ('💚', 1040),\n",
       " ('💪', 947)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### q2_memory\n",
    "\n",
    "Se realiza una prueba local, quedando pendiente realizar la prueba en un cluster dataproc.\n",
    "\n",
    "Los pasos de ejecución son:\n",
    "\n",
    "Se definen dos patrones de regex:\n",
    "\n",
    "    - emoji_pattern: Rango Unicode que incluye varias categorías de emojis.\n",
    "    - skin_tone_modifiers: Rango que captura modificadores de tono de piel.\n",
    "\n",
    "Se define la funcion extract_emojis:\n",
    "\n",
    "    - Elimina los modificadores de tono de piel para que no afecten los conteos.\n",
    "    - Extrae los emojis usando el patrón emoji_pattern.\n",
    "\n",
    "Se crea un rdd\n",
    "\n",
    "    rdd = df.select(\"content\").rdd.flatMap(lambda row: extract_emojis(row[\"content\"]))\n",
    "\n",
    "Se realiza conteo de emojis\n",
    "\n",
    "    emoji_counts = (\n",
    "        rdd.filter(lambda emoji: emoji)           # Filtra emojis válidos\n",
    "        .map(lambda emoji: (emoji, 1))           # Mapea cada emoji a (emoji, 1)\n",
    "        .reduceByKey(lambda a, b: a + b)         # Suma las ocurrencias de cada emoji\n",
    "    )\n",
    "\n",
    "    filter: Asegura que solo se procesen valores no vacíos.\n",
    "    map: Convierte cada emoji en una tupla (emoji, 1) para contar las ocurrencias.\n",
    "    reduceByKey: Agrupa los emojis y suma sus ocurrencias para calcular el conteo total.\n",
    "\n",
    "Se obtienen los 10 emojis más usados\n",
    "\n",
    "    top_10_emojis = emoji_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-23T06:59:34.356946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from q2_memory import q2_memory\n",
    "\n",
    "file_path = \"/Users/acarcamo/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "q2_memory(file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PROBLEMA 3: El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos\n",
    "\n",
    "\n",
    "### q3_time\n",
    "\n",
    "Revisando la data se verifica que el campo mentionedUsers.username contiene los usuarios mencionados en cada tweet. Para esto la query consiste de 2 pasos:\n",
    "\n",
    "   Desanidar la lista de usuarios mencionados en una estructura tabular\n",
    "\n",
    "        WITH mentioned_users AS (\n",
    "          SELECT\n",
    "            A.id,\n",
    "            mentionedUsers.username AS mention\n",
    "          FROM {dataset_id}.{table_id} A,\n",
    "          UNNEST(mentionedUsers) AS mentionedUsers\n",
    "        )\n",
    "   Contar cuántas veces fue mencionado cada usuario y obtener los 10 más mencionados.\n",
    "\n",
    "        SELECT\n",
    "          mention AS username,\n",
    "          COUNT(*) AS mention_count\n",
    "        FROM mentioned_users\n",
    "        GROUP BY mention\n",
    "        ORDER BY mention_count DESC\n",
    "        LIMIT 10;\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T07:09:45.774968Z",
     "start_time": "2024-11-23T07:08:43.206223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from q3_time import q3_time\n",
    "\n",
    "file_path = \"/Users/acarcamo/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "q3_time(file_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados en Bigquery\n",
      "Filename: /Users/acarcamo/Documents/Personal/challenge_DE/src/q3_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     9     86.0 MiB     86.0 MiB           1   @profile\n",
      "    10                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    11     86.0 MiB      0.0 MiB           1       dataset_id = \"sandbox_agcarcamo\"\n",
      "    12     86.0 MiB      0.0 MiB           1       table_id = \"de_test3\"\n",
      "    13                                         \n",
      "    14     86.0 MiB      0.0 MiB           1       result = []\n",
      "    15                                         \n",
      "    16     86.0 MiB      0.0 MiB           1       try:\n",
      "    17                                                 # Valida si el archivo existe\n",
      "    18     86.0 MiB      0.0 MiB           1           if not os.path.exists(file_path):\n",
      "    19                                                     raise FileNotFoundError(f\"El archivo {file_path} no se encuentra.\")\n",
      "    20                                         \n",
      "    21     86.4 MiB      0.4 MiB           1           client = bigquery.Client()\n",
      "    22                                         \n",
      "    23                                                 # Lee el json\n",
      "    24   1103.5 MiB      0.0 MiB           2           with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
      "    25   1103.5 MiB   1017.2 MiB      117408               json_data = [json.loads(line) for line in file]\n",
      "    26                                         \n",
      "    27                                                 # Configuracion del Job\n",
      "    28   1103.5 MiB      0.0 MiB           2           job_config = bigquery.LoadJobConfig(\n",
      "    29   1103.5 MiB      0.0 MiB           1               source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
      "    30   1103.5 MiB      0.0 MiB           1               autodetect=True,  # Automatically detect the schema\n",
      "    31   1103.5 MiB      0.0 MiB           1               max_bad_records=10,\n",
      "    32   1103.5 MiB      0.0 MiB           1               write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Or WRITE_TRUNCATE, based on your needs\n",
      "    33                                                 )\n",
      "    34                                         \n",
      "    35                                                 # Job para cargar la data\n",
      "    36   2179.0 MiB   1075.5 MiB           2           load_job = client.load_table_from_json(\n",
      "    37   1103.5 MiB      0.0 MiB           1               json_data, dataset_id + '.' + table_id, job_config=job_config\n",
      "    38                                                 )\n",
      "    39                                         \n",
      "    40   2180.2 MiB      1.2 MiB           1           load_job.result()\n",
      "    41                                         \n",
      "    42   2180.6 MiB      0.4 MiB           1           print(\"Datos cargados en Bigquery\")\n",
      "    43                                         \n",
      "    44                                                 # Busca la query\n",
      "    45   2180.6 MiB      0.0 MiB           1           sql_file_path = os.path.join(os.path.dirname(__file__), \"queries\", \"q3_time.sql\")\n",
      "    46   2181.4 MiB      0.7 MiB           1           if not os.path.exists(sql_file_path):\n",
      "    47                                                     raise FileNotFoundError(f\"El archivo SQL {sql_file_path} no se encuentra.\")\n",
      "    48                                         \n",
      "    49   2181.4 MiB      0.0 MiB           2           with open(sql_file_path, \"r\", encoding=\"utf-8\") as sql_file:\n",
      "    50   2181.4 MiB      0.0 MiB           1               query_template = sql_file.read()\n",
      "    51                                         \n",
      "    52                                                 #  Setea dataset_id y table_id en la query\n",
      "    53   2181.4 MiB      0.0 MiB           1           try:\n",
      "    54   2181.4 MiB      0.0 MiB           1               query = query_template.format(dataset_id=dataset_id, table_id=table_id)\n",
      "    55                                                 except KeyError as e:\n",
      "    56                                                     print(f\"Error formatting the query: {e}\")\n",
      "    57                                                     return result\n",
      "    58                                         \n",
      "    59   2182.9 MiB      1.5 MiB           1           query_job = client.query(query)\n",
      "    60   2183.4 MiB      0.5 MiB           1           results = query_job.result()\n",
      "    61                                         \n",
      "    62                                                 # Armar lista de salida\n",
      "    63   2183.5 MiB      0.1 MiB          11           for row in results:\n",
      "    64   2183.5 MiB      0.0 MiB          10               username = row['username']\n",
      "    65   2183.5 MiB      0.0 MiB          10               mention_count = row['mention_count']\n",
      "    66   2183.5 MiB      0.0 MiB          10               result.append((username, mention_count))\n",
      "    67                                         \n",
      "    68                                             except FileNotFoundError as e:\n",
      "    69                                                 print(f\"Error: {e}\")\n",
      "    70                                             except GoogleAPIError as e:\n",
      "    71                                                 print(f\"Error en BigQuery: {e}\")\n",
      "    72                                             except Exception as e:\n",
      "    73                                                 print(f\"Error no esperado: {e}\")\n",
      "    74                                         \n",
      "    75   2183.5 MiB      0.0 MiB           1       return result\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### q3_memory\n",
    "\n",
    "Se realiza una prueba local, quedando pendiente realizar la prueba en un cluster dataproc.\n",
    "\n",
    "Los pasos de ejecución son:\n",
    "\n",
    "Se carga la data a un dataframe:\n",
    "\n",
    "    df = spark.read.option(\"encoding\", \"UTF-8\").json(file_path)\n",
    "\n",
    "\n",
    "Se realiza un explode de las menciones. Por cada usuario dentro de mentionedUsers.username se crea un registro en la nueva columna mention.\n",
    "\n",
    "    flattened_df = df.withColumn(\"mention\", explode(col(\"mentionedUsers.username\")))\n",
    "\n",
    "\n",
    "Se cuenta la cantidad de menciones por usuario\n",
    "\n",
    "    mention_count_df = flattened_df.groupBy(\"mention\").agg(F.count(\"*\").alias(\"mention_count\"))\n",
    "\n",
    "Se obtienen los 10 usuarios mas influyentes\n",
    "\n",
    "    top_users = mention_count_df.orderBy(\"mention_count\", ascending=False).limit(10)\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T07:11:19.242972Z",
     "start_time": "2024-11-23T07:10:36.536974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from q3_memory import q3_memory\n",
    "\n",
    "file_path = \"/Users/acarcamo/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "q3_memory(file_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/23 04:11:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=23520Kb max_used=23520Kb free=107551Kb\n",
      " bounds [0x00000001064d8000, 0x0000000107bf8000, 0x000000010e4d8000]\n",
      " total_blobs=9103 nmethods=8197 adapters=819\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
